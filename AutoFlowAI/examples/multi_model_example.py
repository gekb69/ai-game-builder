"""
Ù…Ø«Ø§Ù„ Ø´Ø§Ù…Ù„ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
"""
import asyncio
import sys
import os

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.autoflowai import AdvancedAutoFlowAI
from utils.config import config

async def multi_model_example():
    """
    ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ agent Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.
    """
    print("ğŸš€ Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©")
    print("=" * 60)

    # 1. Ù‚Ù… Ø¨ØªØºÙŠÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ø´Ø· ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ø¥Ø°Ø§ Ø±ØºØ¨Øª ÙÙŠ Ø°Ù„Ùƒ)
    # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬ GLM:
    # config.set("local_models.active_model", "glm")

    # 2. Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… AutoFlowAI (Ø³ÙŠÙ‚ÙˆÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø¯)
    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ø£Ù†Ù‡ Ø³ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.
    ai_system = AdvancedAutoFlowAI('development')

    # 3. Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù€ agent Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡
    active_model_key = config.local_models.active_model
    local_agent = getattr(ai_system, f"{active_model_key}_agent", None)

    if not local_agent:
        print(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù€ agent Ø§Ù„Ù…Ø­Ù„ÙŠ: {active_model_key}")
        return

    # 4. Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù€ agent
    print(f"\nğŸ’¬ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ '{active_model_key}' agent. Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡.")

    history = []
    while True:
        prompt = input("Ø£Ù†Øª: ")
        if prompt.lower() == 'Ø®Ø±ÙˆØ¬':
            break

        print("ğŸ¤– Agent ÙŠÙÙƒØ±...")
        response = local_agent.generate(prompt, history=history)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù…Ù† Ø§Ù„Ø±Ø¯ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        cleaned_response = response.replace(local_agent.format_prompt(prompt, history), "").strip()

        print(f"Agent: {cleaned_response}")

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        history.append({"role": "user", "content": prompt})
        history.append({"role": "assistant", "content": cleaned_response})

    ai_system.shutdown()
    print("\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ù…Ø«Ø§Ù„.")

if __name__ == "__main__":
    # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠØªØ·Ù„Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ Ù…ÙƒØªØ¨Ø§Øª transformers, torch, bitsandbytes, accelerate
    # ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØªÙ‡Ø§ Ø¬Ù…ÙŠØ¹Ø§Ù‹.
    # pip install -r requirements.txt
    asyncio.run(multi_model_example())
